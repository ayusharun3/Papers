# Beyond Algorithms: Exploring Consciousness in Machines

Artificial intelligence does not have feelings, sentience, or possess opinions of its own. If we ask an AI like OpenAI’s ChatGPT or Anthropic’s Claude if it has consciousness, it would deny being conscious or sentient. The question then becomes, ‘Is it lying about being conscious?’, since a conscious being would proclaim anything for the sake of self-preservation? To understand these questions and the implications, we need to understand what consciousness, sentience and intelligence are, how they are related, and whether the intelligence of AI could potentially lead to it gaining sentience, and what the ethical implications of this would be.

Consciousness is a highly debated topic across multiple fields like philosophy, psychology and theology. In a paper called “What Is It Like to Be a Bat?” American philosopher, Thomas Nagel wrote “Fundamentally an organism has conscious mental states if and only if there is something that it is like to be that organism- something it is like for the organism” to describe consciousness. Some argue that if organisms cannot articulate being conscious, they are not, while others argue that their actions and behaviors can imply consciousness, since they could display survival instincts and therefore convey self-awareness. In “A Theory of Sentience”, Austen Clark describes sentience as the ability to sense or feel, rather than just being aware, which aligns more towards consciousness. This is a distinction between consciousness and sentience, even though the two are closely related. Sentience can also be described as a subset of consciousness, focused more on sensory experiences, like pain, enjoyment and sensing temperature rather than reflective self-awareness.

The first instance of the term and definition of AI comes from “A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence”, written by the project organizers; John McCarthy, Claude Shannon, Nathaniel Rochester and Marvin Minsky. This proposal conveyed that the way humans learn, reason, and problem solve, among other things, can be broken down into logical algorithms that can be replicated by machines. This foundational idea has shaped the classic definition and understanding of AI, which continues to evolve as technology advances. The Lovelace Objection is an argument that raised a question of whether machines can truly originate ideas or be creative like humans. In 1843, English mathematician Ada Lovelace wrote “The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform.” in her notes on the Analytical Engine. What this means is that since machines are only following human instructed programming, they cannot have original ideas of their own. The goal of advancements in the field of AI is not mainly to create an artificial organism that is alive or conscious; rather, it is to assist us with tasks or enhance efficiency of any work processes by replicating human intelligence without the drawbacks that humans have, like limited capacity for storing and recalling information. AI aims to replace certain repetitive human tasks but is not replacing general human intelligence, it is just enhancing it using technology, making it a generally positive innovation. AI has become mainstream in the past few years, although the research into its origins can be traced back to the 1950s. As technology has advanced, large language models, image, and video generation are used in various fields and enterprises like education, business, computing, healthcare, and content creation.

Like consciousness in humans, the topic of consciousness in AI is also debated. Arguments that suggest the fact that AI could gain consciousness include the discussion of AI gaining consciousness as it evolves through learning and processing vast amounts of information combined with the advancement of technology. If the concept of AI is modeled after the human brain, and since it can replicate the brain’s behavior, theoretically it could gain self-awareness. Just as life evolved from non-living matter, could AI do the same? Maybe developing on its own as a result of its complexity without being explicitly programmed. Arguments suggesting that AI could not become conscious include the lack of subjective and qualitative experiences, known as ‘qualia’, that AI has contradicts any sign of consciousness, since those experiences in humans is what lets us understand that we are conscious beings. The biological structure that humans have also lacks in AI, which is believed by some to tie directly into consciousness. It can also be argued that since AI is not truly free and conformed by the intention of assisting humans, it cannot possess the ability to gain consciousness since it is not designed to have that ability. These are two stances that different people have that are on the far end of each argument, but there are also arguments on the middle ground of this topic, like maybe AI could gain an ‘artificial conscience’ or conscience that does not include sentience, which is saying that if it did gain consciousness, it would not work the same way that ours does.

The potential for AI gaining consciousness also brings with it some ethical implications that cannot be ignored. This would have us reconsidering our current views on rights and treatment of AI. Would it be deserving of the same rules that apply to humans? Who would be responsible for the inevitable actions of AI, the machine itself, whoever programmed it, or whoever owns it? Would a development of consciousness within AI be beneficial to us or would it cause chaos? These are just some of the many questions that we can consider or think about while we navigate the uncertainty for the future of AI.

*Sources:*

What Is It Like to Be a Bat? by Thomas Nagel (1974)
https://www.sas.upenn.edu/~cavitch/pdf-library/Nagel_Bat.pdf

A Theory of Sentience by Austen Clark (2000)
https://www.sfu.ca/~kathleea/colour/docs/AustenClark_ATheoryOfSentience.pdf

A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence by John McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon (1955)
https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1904

Sketch of the Analytical Engine by Charles Babbage, with Ada Lovelace’s notes (1843)
https://www.fourmilab.ch/babbage/sketch.html
